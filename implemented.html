<!doctype html>
<html lang="en">
<meta charset="utf-8">
<title>knime2py — Implemented Nodes</title>
<style>
  body { font-family: system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif;
         padding: 24px; color: #222; background: #fff; }
  h1 { margin: 0 0 12px; font-size: 20px; }
  .meta { color: #666; margin: 0 0 18px; font-size: 13px; }
  table { border-collapse: collapse; width: 100%; }
  th, td { border: 1px solid #ddd; padding: 8px 10px; vertical-align: top; }
  th { background: #f6f6f6; text-align: left; }
  tr:nth-child(even) td { background: #fafafa; }
  code { background: #f3f3f3; padding: 1px 4px; border-radius: 4px; }
</style>
<body>
<h1>knime2py — Implemented Nodes</h1>
<p class="meta">This page lists the KNIME nodes currently supported for code generation. 
For unsupported nodes, the generator produces code that initializes node parameters from settings.xml. </p>
<table>
  <thead>
    <tr>
      <th>KNIME Node</th>
      <th>Module</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>Column Filter</td><td><code>column_filter.py</code></td><td>Supports multiple KNIME factories:<br>- org.knime.base.node.preproc.filter.column2.ColumnFilter2NodeFactory (newer)<br>- org.knime.base.node.preproc.colfilter.ColumnFilterNodeFactory (classic)<br>- org.knime.base.node.preproc.filter.column.DataColumnSpecFilterNodeFactory (legacy/alt)<br>Parsing heuristics: looks for &lt;config&gt; blocks whose keys contain &quot;include&quot;/&quot;exclude&quot;, numeric<br>index entries (&lt;entry key=&#x27;0&#x27; value=&#x27;Col&#x27;/&gt; ...), or name entries (&lt;entry key=&#x27;name&#x27; value=&#x27;Col&#x27;/&gt;).<br>Falls back to generic &quot;columns&quot; blocks if include/exclude buckets are absent. Duplicate names are<br>de-duplicated while preserving the first occurrence. If no includes/excludes are found, the node<br>is a passthrough. Excludes are dropped with errors=&#x27;ignore&#x27;. Only explicit column-name lists are<br>supported—pattern/type/regex-based selection is not implemented. Depends on lxml for parsing and<br>emits pandas-only code; relies on project utilities (iter_entries, normalize_in_ports, etc.).</td></tr>
    <tr><td>CSV Reader</td><td><code>csv_reader.py</code></td><td>pandas&gt;=1.5 recommended (nullable dtypes supported in dtype mapping).<br>Quote/escape are passed to pandas. If escapechar is None, pandas&#x27; default engine behavior applies.<br>Dtype mapping is derived from table_spec_config_Internals; unknown types are left to inference.<br>Path resolution supports LOCAL and RELATIVE knime.workflow only; other FS types are not yet handled.</td></tr>
    <tr><td>CSV Writer</td><td><code>csv_writer.py</code></td><td>pandas&gt;=1.5 recommended for consistent NA/nullable dtype handling.<br>Path resolution supports LOCAL absolute paths and RELATIVE knime.workflow; other FS types are not yet handled.<br>Directory creation is not automatic; ensure out_path.parent exists before writing.<br>Line terminator / quoting mode / doublequote / escapechar are not explicitly mapped unless present; pandas defaults apply.<br>File is overwritten by default; KNIME “append/overwrite” style flags are not implemented here.</td></tr>
    <tr><td>Decision Tree Learner</td><td><code>decision_tree_learner.py</code></td><td>Pruning options (e.g., pruningMethod/Reduced Error Pruning) are not available in sklearn DT;<br>consider ccp_alpha for cost-complexity pruning if needed.<br>First-split constraints and binary nominal split settings are not supported by sklearn.<br>Feature importances are impurity-based (Gini/entropy); consider permutation importances if<br>you need model-agnostic measures.<br>Library expectations: pandas&gt;=1.5, numpy&gt;=1.23, scikit-learn&gt;=1.2 recommended.</td></tr>
    <tr><td>Decision Tree Predictor</td><td><code>decision_tree_predictor.py</code></td><td>The estimator itself must be scikit-learn-like.<br>Scope: classification predictor only. Multi-output and regression variants are not handled</td></tr>
    <tr><td>Equal Size Sampling</td><td><code>equal_size_sampling.py</code></td><td>Exact mode only: “Approximate” sampling is not implemented in this generator.<br>Requires pandas; scikit-learn is used only for resample() (no synthetic example generation).<br>Seed is used when provided; default fallback is 1 for deterministic output.<br>Order of rows after concatenation is re-sorted back to the original index.</td></tr>
    <tr><td>Gradient Boosted Trees (Classification) Learner</td><td><code>gbt_learner.py</code></td><td>- Feature selection: use included_names if present; otherwise all numeric/boolean columns except<br>the target. Excluded_names are removed afterward. If no target is configured, the node is a<br>passthrough: bundle=None and empty outputs with an error note in the summary.<br>- Hyperparameters mapped: nrModels→n_estimators, learningRate→learning_rate, maxLevels<br>(-1/absent → default 3)→max_depth, minNodeSize→min_samples_split (≥2), minChildSize→min_samples_leaf (≥1),<br>dataFraction (0&lt;≤1)→subsample (stochastic GB), columnSamplingMode→max_features (None/&#x27;sqrt&#x27;/&#x27;log2&#x27;/fraction/int),<br>seed→random_state. Seed defaults to 1 for deterministic output.<br>- Unsupported/orthogonal flags: splitCriterion (trees in sklearn GBT have fixed criterion),<br>missingValueHandling (impute beforehand), useAverageSplitPoints, useBinaryNominalSplits,<br>isUseDifferentAttributesAtEachNode (no direct sklearn analog). These are noted and ignored.<br>- Outputs: port 1=model bundle (estimator, metadata), port 2=feature_importances_, port 3=summary.<br>- Dependencies: lxml for XML parsing; pandas/numpy for data handling; scikit-learn for modeling.</td></tr>
    <tr><td>Gradient Boosted Trees (Classification) Predictor</td><td><code>gbt_predictor.py</code></td><td>Notes:<br>- Bundle keys (if present): {&#x27;estimator&#x27;,&#x27;features&#x27;,&#x27;target&#x27;,&#x27;classes&#x27;,...}; falls back gracefully<br>to bare estimator and infers features if needed (raises KeyError if required columns are missing).<br>- Prediction column name: custom if configured, else &quot;Prediction (&lt;target&gt;)&quot;.<br>- Probabilities: adds per-class &quot;P (&lt;target&gt;=&lt;class&gt;)&lt;suffix&gt;&quot; when predict_proba is available; may<br>also append &quot;&lt;prediction&gt; (confidence)&quot; as max probability.<br>- Optional: append number of boosted estimators as &quot;&lt;prediction&gt; (models)&quot;.<br>- Ignored flag: &#x27;useSoftVoting&#x27; (not applicable to sklearn GBT).</td></tr>
    <tr><td>Logistic Regression Learner</td><td><code>logreg_learner.py</code></td><td>- Feature selection: use included_names if set; otherwise all numeric/boolean columns minus the<br>target; then remove excluded_names.<br>- Hyperparameter mapping: solver(KNIME→sklearn), maxEpoch→max_iter, epsilon→tol, seed→random_state.<br>Target reference category is recorded as metadata only (no sklearn equivalent).</td></tr>
    <tr><td>Logistic Regression Predictor</td><td><code>logreg_predictor.py</code></td><td>- Bundle keys (if present): {&#x27;estimator&#x27;,&#x27;features&#x27;,&#x27;target&#x27;,&#x27;classes&#x27;,...}; falls back to a bare<br>estimator and infers features if absent (raises KeyError if required columns are missing).<br>- Prediction column name: custom if configured; otherwise &quot;Prediction (&lt;target&gt;)&quot;.<br>- Probabilities: when predict_proba exists, adds &quot;P (&lt;target&gt;=&lt;class&gt;)&lt;suffix&gt;&quot; columns.<br>- XML quirks: reads KNIME’s misspelled keys verbatim<br>(has_custom_predicition_name, include_probabilites, propability_columns_suffix).</td></tr>
    <tr><td>Missing Value Handler</td><td><code>missing_value.py</code></td><td>- Strategies (by dtype): fixed, mean, median, mode, ffill, bfill, drop. Fixed values are read<br>from common keys (fixIntegerValue, fixLongValue, fixDoubleValue, fixStringValue, fixBooleanValue, fixValue)<br>and emitted as literals when possible (with safe casting for ints/floats).<br>- Dtype mapping: IntCell/LongCell→int, DoubleCell→float, StringCell→string, BooleanCell→boolean.<br>- Scope: type-wide policies from dataTypeSettings; per-column policies are not implemented here.</td></tr>
    <tr><td>Normalizer</td><td><code>normalizer.py</code></td><td>- Column selection: use included_names if set; else all numeric dtypes (Int*/int*/Float*/float*);<br>drop excluded_names afterward.<br>- Modes: MINMAX uses new-min/new-max (constant/empty columns map to new_min);<br>ZSCORE uses (x-mean)/std (zero std → 0.0).</td></tr>
    <tr><td>Partitioning</td><td><code>partitioning.py</code></td><td>- Implementation: sklearn.model_selection.train_test_split; seed honored when provided.<br>- STRATIFIED: uses class_column; NaN treated as a separate class; falls back to non-stratified if<br>stratification is infeasible (e.g., tiny classes).<br>- RELATIVE: fraction is clamped to [0,1]. ABSOLUTE: train_size is an integer bounded by len(df).</td></tr>
    <tr><td>Random Forest (Classification) Learner</td><td><code>random_forest_learner.py</code></td><td>- Feature selection: use included_names if provided; otherwise all numeric/boolean columns except<br>the target; excluded_names are removed afterward.<br>- Hyperparameter mapping: nrModels→n_estimators; maxLevels&gt;0→max_depth else None; minNodeSize→min_samples_split;<br>minChildSize→min_samples_leaf; isDataSelectionWithReplacement→bootstrap; dataFraction→max_samples<br>(only when bootstrap=True); columnSamplingMode/columnFractionPerTree/columnAbsolutePerTree plus<br>isUseDifferentAttributesAtEachNode→max_features (&#x27;sqrt&#x27;/&#x27;log2&#x27;/1.0/fraction/int); seed→random_state.<br>- Info-only flags (not applied in sklearn RF): splitCriterion, missingValueHandling,<br>useAverageSplitPoints, useBinaryNominalSplits; noted and ignored.</td></tr>
    <tr><td>Random Forest (Classification) Predictor</td><td><code>random_forest_predictor.py</code></td><td>- Ports: In1=model bundle, In2=data table, Out1=predicted table.<br>- Bundle keys (if present): {&#x27;estimator&#x27;,&#x27;features&#x27;,&#x27;target&#x27;,&#x27;classes&#x27;,...}; falls back to a bare<br>estimator and infers features if absent (raises KeyError if required columns are missing).<br>- Prediction column name: custom if configured; otherwise &quot;Prediction (&lt;target&gt;)&quot;.<br>- Probabilities: when available, adds &quot;P (&lt;target&gt;=&lt;class&gt;)&lt;suffix&gt;&quot;; may also append<br>&quot;&lt;prediction&gt; (confidence)&quot; as max probability. Optional &quot;Model Count&quot; from n_estimators.<br>- &#x27;useSoftVoting&#x27; is informational; sklearn RandomForest averages probabilities by design.</td></tr>
    <tr><td>ROC Curve</td><td><code>roc_curve.py</code></td><td>- Inputs: one table with a truth column and per-class probability columns.<br>- Column binding: uses configured target/positive class and selected probability columns; if none<br>are set, attempts to auto-detect KNIME-style columns like &quot;P (&lt;target&gt;=&lt;class&gt;)_LR&quot;. Configure<br>explicitly if your suffix differs (e.g., _RF, _GB).<br>- Output artifacts: saves &quot;roc_&lt;node_id&gt;.(png|svg)&quot; and &quot;roc_table_&lt;node_id&gt;.csv&quot; in CWD; figure<br>size from width/height (pixels at 100 DPI). Title/axis labels are honored from settings.<br>- Implementation: sklearn.metrics.roc_curve/auc for each probability series; matplotlib for<br>plotting; pandas/numpy for data handling.</td></tr>
    <tr><td>Rule Engine</td><td><code>rule_engine.py</code></td><td>- Supported rules: TRUE =&gt; &quot;out&quot;; $col$ &lt;op&gt; value =&gt; &quot;out&quot; with &lt;, &lt;=, &gt;, &gt;=, =, ==, !=;<br>$col$ LIKE &quot;pat&quot; (uses * as wildcard; converted to a regex). A trailing TRUE acts as default.<br>- Column output: append to a new column if configured; otherwise replace the specified column;<br>falls back to &quot;RuleResult&quot; when no name is provided.<br>- Literals: numeric strings are emitted as numbers; everything else is a quoted Python literal.<br>- Limitations: no AND/OR chaining, no between/in lists, no regex beyond LIKE→wildcard, and no<br>type coercion beyond basic string/number handling.</td></tr>
    <tr><td>Scorer</td><td><code>scorer.py</code></td><td>- Columns: &#x27;first&#x27; → truth column, &#x27;second&#x27; → prediction column (default &quot;Prediction (&lt;truth&gt;)&quot;).<br>ignore.missing.values=true drops NA before scoring; false keeps NA (sklearn metrics may fail).<br>- Confusion matrix labels: union of values from truth and prediction in order of appearance.</td></tr>
    <tr><td>SMOTE</td><td><code>smote.py</code></td><td>- Feature/target: uses all numeric/bool columns as features and the configured class/target.<br>- Methods:<br>• oversample_equal → sampling_strategy=&#x27;auto&#x27; (minorities up to majority)<br>• otherwise uses rate: (0,1] → target_n ≈ rate * majority_n; &gt;1 → target_n ≈ rate * minority_n<br>- kNN: k_neighbors is clamped to ≤ (minority_count - 1) to avoid imblearn errors.<br>- Fallbacks: if no target, no numeric features, single-class, or SMOTE raises, the original df<br>is returned unchanged.</td></tr>
  </tbody>
</table>
</body>
</html>
