# Auto-generated test for KNIME workflow "HW_Churn_test_preparation"
# Generated by test_gen.cli â€” do not hand-edit; re-generate from the source workflow.

"""
Auto-generated test for the KNIME workflow "HW_Churn_test_preparation".

Overview
----------------------------
This module tests the roundtrip generation and execution of the KNIME workflow 
"HW_Churn_test_preparation". It verifies that the workflow can be successfully 
converted to a Python workbook, executed, and that the output matches the 
expected CSV files.

Runtime Behavior
----------------------------
Inputs:
- The module reads the KNIME workflow file and expects to find input DataFrames 
  corresponding to the defined ports.

Outputs:
- The generated code writes output CSV files to the specified output directory, 
  with port mapping based on the workflow configuration.

Key algorithms or mappings:
- The module ensures that the generated Python code adheres to the expected 
  structure and logic defined in the KNIME workflow.

Edge Cases
----------------------------
The code implements safeguards against missing workflow files, empty expected 
output directories, and ensures that the generated output matches the expected 
CSV files.

Generated Code Dependencies
----------------------------
The generated code requires the following external libraries: pandas, numpy, 
and any other libraries specified in the KNIME workflow. These dependencies 
are required by the generated code, not by this test module.

Usage
----------------------------
This module is typically invoked by a test framework, and it expects the 
output directory to be provided by a fixture. An example of expected context 
access is:
```python
output_dir = Path("path/to/output")
```

Node Identity
----------------------------
The module generates code based on the KNIME factory id for the workflow, 
which is defined in the settings.xml file.

Configuration
----------------------------
The settings are parsed using a `@dataclass` that includes important fields 
such as input and output port configurations. The `parse_*` functions extract 
these values from the settings.xml file.

Limitations / Not implemented
----------------------------
This module does not support all KNIME features and may approximate certain 
behaviors.

References
----------------------------
For more information, refer to the KNIME documentation and the HUB_URL constant.
"""

import os
import subprocess
import sys
from pathlib import Path

from support import csv_compare  # provides compare_csv(...) and RTOL

# Resolve RTOL: env K2P_RTOL overrides the library default
RTOL = 0.01

def test_roundtrip_hw_churn_test_preparation(output_dir: Path):
    """
    Test the roundtrip generation and execution of the KNIME workflow 
    "HW_Churn_test_preparation". This function verifies that the workflow 
    can be successfully converted to a Python workbook, executed, and that 
    the output matches the expected CSV files.

    Args:
        output_dir (Path): The directory where the generated Python workbook 
                           and output CSV files will be stored.
    """
    repo_root = Path(__file__).resolve().parents[1]
    knime_proj = repo_root / "tests" / "data" / "HW_Churn_test_preparation"
    out_dir = output_dir  # provided by conftest.py fixture
    expected_dir = repo_root / "tests" / "data" / "data" / "HW_Churn_test_preparation"

    # Preconditions
    assert (knime_proj / "workflow.knime").exists(), f"Missing workflow.knime in {knime_proj}"
    assert expected_dir.exists(), f"Expected directory missing: {expected_dir}"

    expected_csvs = sorted(expected_dir.glob("*output.csv"))
    assert expected_csvs, f"No expected '*output.csv' files found in {expected_dir}. Contents: {[p.name for p in expected_dir.iterdir()]}"

    # 1) Generate Python workbook(s) only, no graphs
    cmd = [
        sys.executable, "-m", "knime2py",
        str(knime_proj),
        "--out", str(out_dir),
        "--graph", "off",
        "--workbook", "py",
    ]
    env = os.environ.copy()
    env["PYTHONPATH"] = str(repo_root / "src") + (os.pathsep + env["PYTHONPATH"] if env.get("PYTHONPATH") else "")
    gen = subprocess.run(cmd, capture_output=True, text=True, cwd=str(repo_root), env=env)
    assert gen.returncode == 0, f"CLI failed\nSTDOUT:\n{gen.stdout}\nSTDERR:\n{gen.stderr}"

    # 2) Locate a generated workbook script
    candidates = sorted(out_dir.glob("*_workbook.py"))
    assert candidates, f"No *_workbook.py generated in {out_dir}. Contents: {[p.name for p in out_dir.iterdir()]}"
    script = candidates[0]

    # 3) Run the generated workbook (cwd=out_dir so relative paths like ../!output/*.csv resolve correctly)
    run = subprocess.run([sys.executable, str(script)], cwd=str(out_dir), capture_output=True, text=True, env=env)
    assert run.returncode == 0, f"Workbook execution failed\nSTDOUT:\n{run.stdout}\nSTDERR:\n{run.stderr}"

    # 4) Compare each expected '*output.csv' with produced file of the same name
    produced_names = {p.name for p in out_dir.glob("*.csv")}
    assert produced_names, f"No produced CSVs in {out_dir}. Contents: {[p.name for p in out_dir.iterdir()]}"

    for exp in expected_csvs:
        produced = out_dir / exp.name
        assert produced.exists(), (
            f"Produced CSV not found for expected '{exp.name}'. "
            f"Produced CSVs: {sorted(produced_names)}"
        )
        csv_compare.compare_csv(produced, exp, rtol=RTOL)
