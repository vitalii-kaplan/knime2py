# Auto-generated test for KNIME workflow "ISU_Master_test_preparation"
# Generated by test_gen.cli — do not hand-edit; re-generate from the source workflow.

import csv
import math
import os
import subprocess
import sys
from pathlib import Path

# Default relative tolerance; override by setting K2P_RTOL, e.g. K2P_RTOL=1e-4
RTOL = float(os.environ.get("K2P_RTOL", "0.001"))

def _read_csv_rows(path: Path):
    """Read CSV into rows (lists of trimmed strings). Skip fully-empty rows."""
    with path.open(newline="") as f:
        reader = csv.reader(f)
        rows = []
        for r in reader:
            if r is None:
                continue
            rr = [(c or "").strip() for c in r]
            if any(rr):  # skip rows that are all empty after trimming
                rows.append(rr)
        return rows

def _try_parse_float(s: str):
    """Return (is_number, value). Accepts inf/-inf/NaN case-insensitively."""
    s2 = (s or "").strip()
    if s2 == "":
        return False, None
    try:
        v = float(s2)
        return True, v
    except Exception:
        low = s2.lower()
        if low in ("nan", "+nan", "-nan"):
            return True, math.nan
        if low in ("inf", "+inf", "infinity", "+infinity"):
            return True, math.inf
        if low in ("-inf", "-infinity"):
            return True, -math.inf
        return False, None

def _cells_equal(a: str, b: str, *, rtol: float) -> bool:
    """Numeric cells equal within RELATIVE tol; strings must match exactly after trimming."""
    an, av = _try_parse_float(a)
    bn, bv = _try_parse_float(b)
    if an and bn:
        # NaN compares equal only if both are NaN
        if math.isnan(av) and math.isnan(bv):
            return True
        # Infinity must match exactly (including sign)
        if math.isinf(av) or math.isinf(bv):
            return av == bv
        # Relative tolerance only (abs_tol = 0)
        return math.isclose(av, bv, rel_tol=rtol, abs_tol=0.0)
    # non-numeric: exact string equality after trim
    return (a or "").strip() == (b or "").strip()

def _compare_csv_with_relative_tolerance(got_path: Path, exp_path: Path, *, rtol: float = RTOL):
    got = _read_csv_rows(got_path)
    exp = _read_csv_rows(exp_path)

    assert len(got) == len(exp), f"Row count differs: got={len(got)}, exp={len(exp)}"
    assert len(got) > 0, "Empty CSV (no header)"

    # Header must match exactly (after trimming)
    assert got[0] == exp[0], f"Header mismatch:\nGOT: {got[0]}\nEXP: {exp[0]}"

    # All data rows: same number and same width per row
    for i, (gr, er) in enumerate(zip(got, exp)):
        assert len(gr) == len(er), f"Column count differs at row {i}: got={len(gr)}, exp={len(er)}"

    # Compare row-by-row, cell-by-cell with relative tolerance
    mismatches = []
    for i in range(1, len(got)):  # skip header row (0)
        gr, er = got[i], exp[i]
        for j, (ga, eb) in enumerate(zip(gr, er)):
            if not _cells_equal(ga, eb, rtol=rtol):
                if len(mismatches) < 25:
                    an, av = _try_parse_float(ga)
                    bn, bv = _try_parse_float(eb)
                    if an and bn and not (math.isnan(av) and math.isnan(bv)) and not (math.isinf(av) or math.isinf(bv)):
                        # Report relative error like math.isclose uses: denom = max(|a|,|b|)
                        diff = abs(av - bv)
                        denom = max(abs(av), abs(bv))
                        if denom == 0.0:
                            rel = math.inf if diff != 0.0 else 0.0
                        else:
                            rel = diff / denom
                        mismatches.append((i, j, ga, eb, rel))
                    else:
                        mismatches.append((i, j, ga, eb, None))
                else:
                    break
        if len(mismatches) >= 25:
            break

    if mismatches:
        lines = [f"First mismatches (row, col, got, exp, rel_err; uses math.isclose rel_tol={rtol}, abs_tol=0):"]
        for m in mismatches:
            i, j, ga, eb, rel = m
            if rel is None or math.isinf(rel):
                lines.append(f"  at ({i},{j}): got={ga!r} exp={eb!r}")
            else:
                lines.append(f"  at ({i},{j}): got={ga!r} exp={eb!r} rel_err≈{rel:.8g}")
        raise AssertionError("\n".join(lines))

def test_roundtrip_isu_master_test_preparation(output_dir: Path):
    repo_root = Path(__file__).resolve().parents[1]
    knime_proj = repo_root / "tests" / "data" / "ISU_Master_test_preparation"
    out_dir = output_dir  # provided by conftest.py fixture
    expected_csv = repo_root / "tests" / "data" / "data" / "ISU_Master_test_preparation" / "output.csv"

    # Preconditions
    assert (knime_proj / "workflow.knime").exists(), f"Missing workflow.knime in {knime_proj}"
    assert expected_csv.exists(), f"Expected reference CSV missing: {expected_csv}"

    # 1) Generate Python workbook(s) only, no graphs
    cmd = [
        sys.executable, "-m", "knime2py",
        str(knime_proj),
        "--out", str(out_dir),
        "--graph", "off",
        "--workbook", "py",
    ]
    env = os.environ.copy()
    env["PYTHONPATH"] = str(repo_root / "src") + (os.pathsep + env["PYTHONPATH"] if env.get("PYTHONPATH") else "")
    gen = subprocess.run(cmd, capture_output=True, text=True, cwd=str(repo_root), env=env)
    assert gen.returncode == 0, f"CLI failed\nSTDOUT:\n{gen.stdout}\nSTDERR:\n{gen.stderr}"

    # 2) Locate a generated workbook script
    candidates = sorted(out_dir.glob("*_workbook.py"))
    assert candidates, f"No *_workbook.py generated in {out_dir}. Contents: {[p.name for p in out_dir.iterdir()]}"
    script = candidates[0]

    # 3) Run the generated workbook (cwd=out_dir so relative paths like ../!output/output.csv resolve correctly)
    run = subprocess.run([sys.executable, str(script)], cwd=str(out_dir), capture_output=True, text=True, env=env)
    assert run.returncode == 0, f"Workbook execution failed\nSTDOUT:\n{run.stdout}\nSTDERR:\n{run.stderr}"

    # 4) Compare the produced CSV to the expected CSV (with RELATIVE tolerance)
    produced_csv = out_dir / "output.csv"
    assert produced_csv.exists(), f"Produced output.csv not found in {out_dir}. Contents: {[p.name for p in out_dir.iterdir()]}"

    _compare_csv_with_relative_tolerance(produced_csv, expected_csv, rtol=RTOL)
